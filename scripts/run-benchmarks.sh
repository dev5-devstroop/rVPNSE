#!/bin/bash
# Comprehensive benchmark runner and report generator

set -euo pipefail

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Configuration
BENCHMARK_DIR="target/criterion"
REPORT_DIR="benchmark-reports"
DATE=$(date +"%Y-%m-%d_%H-%M-%S")
COMMIT_HASH=$(git rev-parse --short HEAD 2>/dev/null || echo "unknown")

echo -e "${BLUE}🚀 RVPNSE Benchmark Suite${NC}"
echo -e "${BLUE}=========================${NC}"
echo "Date: $(date)"
echo "Commit: $COMMIT_HASH"
echo ""

# Create report directory
mkdir -p "$REPORT_DIR"

# Function to run a specific benchmark
run_benchmark() {
    local bench_name=$1
    local description=$2
    
    echo -e "${YELLOW}📊 Running $description...${NC}"
    
    # Simple benchmark run without output format flags that might not be supported
    if cargo bench --bench "$bench_name" ${QUICK_MODE:+-- --quick}; then
        echo -e "${GREEN}✅ $description completed${NC}"
        return 0
    else
        echo -e "${RED}❌ $description failed${NC}"
        return 1
    fi
}

# Function to generate summary report
generate_summary() {
    local report_file="$REPORT_DIR/benchmark-summary-$DATE.md"
    
    cat > "$report_file" << EOF
# RVPNSE Benchmark Report

**Generated**: $(date)  
**Commit**: $COMMIT_HASH  
**Platform**: $(uname -s) $(uname -m)  
**Rust Version**: $(rustc --version)  

## Executive Summary

This report contains performance benchmarks for the RVPNSE static library,
measuring key operations across configuration parsing, client operations,
and FFI interface performance.

## Benchmark Results

EOF

    # Add individual benchmark summaries if available
    if [ -d "$BENCHMARK_DIR" ]; then
        echo "### Available Benchmark Data" >> "$report_file"
        echo "" >> "$report_file"
        find "$BENCHMARK_DIR" -name "*.html" | while read -r html_file; do
            local bench_name=$(basename "$(dirname "$html_file")")
            echo "- [$bench_name]($(basename "$html_file"))" >> "$report_file"
        done
        echo "" >> "$report_file"
    fi

    cat >> "$report_file" << EOF

## Performance Categories

### Configuration Parsing
- **TOML Parsing**: Measures configuration file parsing performance
- **Validation**: Configuration validation overhead
- **Serialization**: Configuration serialization performance
- **Throughput**: Parser throughput across different config sizes

### Client Operations  
- **Creation**: VPN client instantiation performance
- **Connection**: Network connection establishment timing
- **Authentication**: Authentication protocol overhead
- **State Management**: Session and state tracking performance
- **Memory Usage**: Memory allocation patterns and efficiency

### FFI Interface
- **Function Calls**: C interface call overhead
- **String Operations**: String marshaling and conversion performance
- **Memory Safety**: Pointer validation and memory management
- **Throughput**: Data processing rates across the FFI boundary

## Platform Information

- **OS**: $(uname -s) $(uname -r)
- **Architecture**: $(uname -m)
- **Rust**: $(rustc --version)
- **Cargo**: $(cargo --version)

## Recommendations

1. **Monitor Trends**: Track performance changes over time
2. **Regression Testing**: Set up automated performance regression detection
3. **Optimization**: Focus on areas showing performance degradation
4. **Platform Testing**: Ensure consistent performance across platforms

---

*Generated by RVPNSE benchmark suite*
EOF

    echo -e "${GREEN}📄 Summary report generated: $report_file${NC}"
}

# Function to generate JSON metrics for CI/CD
generate_metrics() {
    local metrics_file="$REPORT_DIR/metrics-$DATE.json"
    
    cat > "$metrics_file" << EOF
{
    "generated_at": "$(date -Iseconds)",
    "commit": "$COMMIT_HASH",
    "platform": {
        "os": "$(uname -s)",
        "arch": "$(uname -m)",
        "rust_version": "$(rustc --version)"
    },
    "benchmarks": {
        "config_parsing": {
            "status": "completed",
            "metrics_available": $([ -d "$BENCHMARK_DIR/config_benchmarks" ] && echo "true" || echo "false")
        },
        "client_operations": {
            "status": "completed", 
            "metrics_available": $([ -d "$BENCHMARK_DIR/client_benchmarks" ] && echo "true" || echo "false")
        },
        "ffi_interface": {
            "status": "completed",
            "metrics_available": $([ -d "$BENCHMARK_DIR/ffi_benchmarks" ] && echo "true" || echo "false")
        }
    }
}
EOF

    echo -e "${GREEN}📊 Metrics generated: $metrics_file${NC}"
}

# Main execution
main() {
    # Build the project first
    echo -e "${YELLOW}🔨 Building project...${NC}"
    if cargo build --release --all-features; then
        echo -e "${GREEN}✅ Build successful${NC}"
    else
        echo -e "${RED}❌ Build failed${NC}"
        exit 1
    fi
    
    echo ""
    
    # Run benchmarks
    local success_count=0
    local total_count=3
    
    if run_benchmark "config_benchmarks" "Configuration Parsing Benchmarks"; then
        ((success_count++))
    fi
    
    if run_benchmark "client_benchmarks" "Client Operations Benchmarks"; then
        ((success_count++))
    fi
    
    if run_benchmark "ffi_benchmarks" "FFI Interface Benchmarks"; then
        ((success_count++))
    fi
    
    echo ""
    echo -e "${BLUE}📈 Benchmark Results Summary${NC}"
    echo -e "${BLUE}=========================${NC}"
    echo "Successful: $success_count/$total_count"
    
    if [ $success_count -eq $total_count ]; then
        echo -e "${GREEN}All benchmarks completed successfully!${NC}"
    else
        echo -e "${YELLOW}Some benchmarks failed. Check output above.${NC}"
    fi
    
    # Generate reports
    echo ""
    echo -e "${YELLOW}📄 Generating reports...${NC}"
    generate_summary
    generate_metrics
    
    # Show final location
    echo ""
    echo -e "${GREEN}🎉 Benchmark suite completed!${NC}"
    echo "Reports available in: $REPORT_DIR/"
    
    if [ -d "$BENCHMARK_DIR" ]; then
        echo "Detailed results in: $BENCHMARK_DIR/"
    fi
}

# Handle script arguments
case "${1:-all}" in
    "config")
        run_benchmark "config_benchmarks" "Configuration Parsing Benchmarks"
        ;;
    "client")
        run_benchmark "client_benchmarks" "Client Operations Benchmarks"
        ;;
    "ffi")
        run_benchmark "ffi_benchmarks" "FFI Interface Benchmarks"
        ;;
    "all"|*)
        main
        ;;
esac
